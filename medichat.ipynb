{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11235005,"sourceType":"datasetVersion","datasetId":7018528}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install rouge-score sacrebleu evaluate torchsummary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T03:38:04.173527Z","iopub.execute_input":"2025-04-01T03:38:04.173846Z","iopub.status.idle":"2025-04-01T03:38:11.780769Z","shell.execute_reply.started":"2025-04-01T03:38:04.173811Z","shell.execute_reply":"2025-04-01T03:38:11.779653Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge-score) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=60bd1a78427cfb4719eb027018c9386210864e1d8a368cbd1127bb8147900074\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: portalocker, sacrebleu, rouge-score, evaluate\nSuccessfully installed evaluate-0.4.3 portalocker-3.1.1 rouge-score-0.1.2 sacrebleu-2.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport evaluate\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments\nfrom transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, TrainerCallback, T5Config\n\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.nn import CrossEntropyLoss\nfrom torch.optim import AdamW\nfrom torch.utils.data import TensorDataset\nfrom torchsummary import summary\n\nfrom collections import defaultdict\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T03:38:33.431994Z","iopub.execute_input":"2025-04-01T03:38:33.432341Z","iopub.status.idle":"2025-04-01T03:38:57.718818Z","shell.execute_reply.started":"2025-04-01T03:38:33.432307Z","shell.execute_reply":"2025-04-01T03:38:57.717762Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Load Data\ndf = pd.read_csv('/kaggle/input/medicaldata/medDataset.csv')\n\n# Sampel Data\nprint(\"Data Sample\")\nprint(df.head())\n\n#Null value\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\n# List of question words\nquestion_words = ['what', 'who', 'why', 'when', 'where', 'how', 'is', 'are', 'does', 'do', 'can', 'will', 'shall']\n\n# Ensure questions are lowercase for consistent filtering\ndf['question'] = df['question'].str.lower()\n\n# Filter rows where the question starts with a question word\ndf = df[df['question'].str.split().str[0].isin(question_words)]\n\ndf = df.reset_index(drop=True)\n\n# Check for duplicate rows\nduplicates = df.duplicated()\nprint(f\"Number of duplicate rows: {duplicates.sum()}\")\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Reset the index after removing duplicates\ndf.reset_index(drop=True, inplace=True)\n\n#Delete Unused column\ndf = df.drop(columns=['source', 'focus_area'])\n\n#Table Info\nprint(\"Table Info\")\nprint(df.info())\n\n# Apply the function\ndf = df.drop_duplicates(subset='question', keep='first').reset_index(drop=True)\ndf = df.drop_duplicates(subset='answer', keep='first').reset_index(drop=True)\n\n#Drop rows with null values\ndf = df.drop_duplicates(subset=['question', 'answer']).reset_index(drop=True)\ndf['question'] = df['question'].fillna('').astype(str)\ndf['answer'] = df['answer'].fillna('').astype(str)\n\n# Removing \"(are)\" in the dataset\ndef clean_text(text):\n    text = re.sub(r\"\\(.*?\\)\", \"\", text)\n    text = re.sub(r'\\s+', ' ', text.strip().lower())\n    return text\n\ndf['question'] = df['question'].apply(clean_text)\ndf['answer'] = df['answer'].apply(clean_text)\n\ndf['question'] = df['question'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\ndf['answer'] = df['answer'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\n\n#Checking again of null values\nprint(\"Null Value Data\")\nprint(df.isnull().sum())\n\n#Check for Unique Data\nprint(f\"Unique questions: {df['question'].nunique()}\")\nprint(f\"Unique answers: {df['answer'].nunique()}\")\n\n#Checking again of the data info\ndf.info()\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T03:42:18.421760Z","iopub.execute_input":"2025-04-01T03:42:18.422059Z","iopub.status.idle":"2025-04-01T03:42:21.114048Z","shell.execute_reply.started":"2025-04-01T03:42:18.422020Z","shell.execute_reply":"2025-04-01T03:42:21.113188Z"}},"outputs":[{"name":"stdout","text":"Data Sample\n                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer           source  \\\n0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n\n  focus_area  \n0   Glaucoma  \n1   Glaucoma  \n2   Glaucoma  \n3   Glaucoma  \n4   Glaucoma  \nNull Value Data\nquestion       0\nanswer         5\nsource         0\nfocus_area    14\ndtype: int64\nNumber of duplicate rows: 48\nTable Info\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 16364 entries, 0 to 16363\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  16364 non-null  object\n 1   answer    16359 non-null  object\ndtypes: object(2)\nmemory usage: 255.8+ KB\nNone\nNull Value Data\nquestion    0\nanswer      0\ndtype: int64\nUnique questions: 13844\nUnique answers: 13852\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13858 entries, 0 to 13857\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   question  13858 non-null  object\n 1   answer    13858 non-null  object\ndtypes: object(2)\nmemory usage: 216.7+ KB\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                 question  \\\n0                      what is glaucoma ?   \n1                  what causes glaucoma ?   \n2     what are the symptoms of glaucoma ?   \n3  what are the treatments for glaucoma ?   \n4          who is at risk for glaucoma? ?   \n\n                                              answer  \n0  glaucoma is a group of diseases that can damag...  \n1  nearly 2.7 million people have glaucoma, a lea...  \n2  symptoms of glaucoma glaucoma can develop in o...  \n3  although open-angle glaucoma cannot be cured, ...  \n4  anyone can develop glaucoma. some people are a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>what is glaucoma ?</td>\n      <td>glaucoma is a group of diseases that can damag...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>what causes glaucoma ?</td>\n      <td>nearly 2.7 million people have glaucoma, a lea...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>what are the symptoms of glaucoma ?</td>\n      <td>symptoms of glaucoma glaucoma can develop in o...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what are the treatments for glaucoma ?</td>\n      <td>although open-angle glaucoma cannot be cured, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>who is at risk for glaucoma? ?</td>\n      <td>anyone can develop glaucoma. some people are a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Load T5-small model and tokenizer\nmodel_name = \"t5-base\"\nconfig = T5Config.from_pretrained(model_name)\nconfig.dropout_rate = 0.1\nconfig.feed_forward_proj = \"gelu\"  \nmodel = T5ForConditionalGeneration.from_pretrained(\n    model_name, \n    config=config\n)\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# Tie weights explicitly\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Print model architecture summary\n# Print detailed model summary\nprint(\"\\nDetailed Model Summary:\")\nprint(\"=\" * 50)\n\ndef summarize_model_by_type(model):\n    layer_summary = defaultdict(int)\n    param_summary = defaultdict(int)\n\n    for name, module in model.named_modules():\n        layer_type = type(module).__name__\n        layer_summary[layer_type] += 1\n        param_summary[layer_type] += sum(p.numel() for p in module.parameters())\n\n    print(f\"{'Layer Type':<30}{'Count':<10}{'Parameters':<15}\")\n    print(\"=\" * 55)\n    for layer_type, count in layer_summary.items():\n        print(f\"{layer_type:<30}{count:<10}{param_summary[layer_type]:<15,}\")\n\nsummarize_model_by_type(model)\n\n# Preprocess function for seq2seq task\ndef preprocess_function(batch):\n    inputs = [f\"answer the following question: {q}\" for q in batch['question']]\n    targets = [f\"{a}\" for a in batch['answer']]\n    \n    model_inputs = tokenizer(\n        inputs,\n        max_length=128,  \n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\",\n    )\n    \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=64,  \n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n    \n    labels[\"input_ids\"][labels[\"input_ids\"] == tokenizer.pad_token_id] = -100\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Train-test split\ntrain_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n# print('Halo: ',val_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T03:42:40.009056Z","iopub.execute_input":"2025-04-01T03:42:40.009433Z","iopub.status.idle":"2025-04-01T03:42:46.740515Z","shell.execute_reply.started":"2025-04-01T03:42:40.009405Z","shell.execute_reply":"2025-04-01T03:42:46.739553Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307f371d2ace47debb0c711023afe8d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b046522ef21143baa9291afbf40e0789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daf98516bec44deea5a65dae3ca0ed21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8b9b4e5177742248eb4f9d184efc71c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e1a6c84d58740f7af59e8da38c563a0"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"\nDetailed Model Summary:\n==================================================\nLayer Type                    Count     Parameters     \n=======================================================\nT5ForConditionalGeneration    1         222,882,048    \nEmbedding                     3         24,653,568     \nT5Stack                       2         247,534,848    \nModuleList                    26        396,455,424    \nT5Block                       24        198,227,712    \nT5LayerSelfAttention          24        56,642,304     \nT5Attention                   36        84,935,424     \nLinear                        193       222,833,664    \nT5LayerNorm                   62        47,616         \nDropout                       86        0              \nT5LayerFF                     24        113,264,640    \nT5DenseActDense               24        113,246,208    \nReLU                          24        0              \nT5LayerCrossAttention         12        28,320,768     \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Preprocess datasets\ntrain_dataset = train_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=32,  \n    remove_columns=train_dataset.column_names,\n    num_proc=4,   \n)\n\nval_dataset = val_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=32,  \n    remove_columns=val_dataset.column_names,\n    num_proc=4,  \n)\n\n\n# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    # eval_steps=1000,  \n    # save_steps=1000,  \n    save_total_limit=2,  \n    learning_rate=5e-4,   \n    num_train_epochs=5,   \n    per_device_train_batch_size=8,   \n    per_device_eval_batch_size=8, \n    lr_scheduler_type=\"cosine_with_restarts\",  \n    warmup_ratio=0.1,  \n    weight_decay=0.05,\n    predict_with_generate=True,\n    fp16=True,   \n    logging_dir=\"./logs\",\n    logging_steps=50,  \n    # load_best_model_at_end=True,\n    metric_for_best_model=\"exact_match\",\n    greater_is_better=True,\n    report_to=\"none\",\n    gradient_accumulation_steps=2,   \n    max_grad_norm=0.5,\n    optim=\"adamw_torch_fused\",  \n    generation_max_length=64,  \n    generation_num_beams=6,\n    dataloader_num_workers=4,   \n    group_by_length=True, \n    remove_unused_columns=True,\n    label_smoothing_factor= 0.1\n)\n\n# training_args.label_smoothing_factor = 0.1\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model,  \n    padding='longest',  \n)\n\n# Create function to show exact match, BLEU and ROUGE\ndef compute_metrics(eval_pred, tokenizer):\n    predictions, labels = eval_pred\n    \n    # Decode predictions\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Normalize text\n    decoded_preds = [text.strip().lower() for text in decoded_preds]\n    decoded_labels = [text.strip().lower() for text in decoded_labels]\n    \n    # Multiple metrics\n    exact_match = np.mean([p == l for p, l in zip(decoded_preds, decoded_labels)])\n    \n    bleu_metric = evaluate.load(\"bleu\")\n    rouge_metric = evaluate.load(\"rouge\")\n    \n    bleu_score = bleu_metric.compute(\n        predictions=decoded_preds, \n        references=[[label] for label in decoded_labels]\n    )[\"bleu\"]\n    \n    rouge_score = rouge_metric.compute(\n        predictions=decoded_preds, \n        references=decoded_labels\n    )[\"rougeL\"]\n    \n    return {\n        \"exact_match\": exact_match,\n        \"BLEU\": bleu_score,\n        \"ROUGE-L\": rouge_score,\n    }\n\n# Initialize data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding='longest',\n    return_tensors=\"pt\"\n)\n\n# Initialize trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T03:43:22.881810Z","iopub.execute_input":"2025-04-01T03:43:22.882131Z","iopub.status.idle":"2025-04-01T03:43:36.568602Z","shell.execute_reply.started":"2025-04-01T03:43:22.882105Z","shell.execute_reply":"2025-04-01T03:43:36.567767Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/11779 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eb2e43159a5423482a9c794973651c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/2079 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a90cb7acc904ea09a90a57bdcf2dba8"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T03:44:22.874696Z","iopub.execute_input":"2025-04-01T03:44:22.875052Z","iopub.status.idle":"2025-04-01T05:01:19.335309Z","shell.execute_reply.started":"2025-04-01T03:44:22.875025Z","shell.execute_reply":"2025-04-01T05:01:19.334268Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1840' max='1840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1840/1840 1:16:51, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Exact Match</th>\n      <th>Bleu</th>\n      <th>Rouge-l</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.989300</td>\n      <td>2.836838</td>\n      <td>0.150072</td>\n      <td>0.301526</td>\n      <td>0.415071</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.823200</td>\n      <td>2.739042</td>\n      <td>0.139971</td>\n      <td>0.317704</td>\n      <td>0.425895</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.705000</td>\n      <td>2.683712</td>\n      <td>0.152477</td>\n      <td>0.321181</td>\n      <td>0.427972</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.508900</td>\n      <td>2.654015</td>\n      <td>0.151996</td>\n      <td>0.324665</td>\n      <td>0.430299</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ae5d75742c438da50dac2afd0bf892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5435de06fce4c069709e639748d5bd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77398294bbc849eb84b6f461edc8e006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c6cf5aef5f84756a583854a1f839422"}},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1840, training_loss=2.7856999563134237, metrics={'train_runtime': 4614.0391, 'train_samples_per_second': 12.764, 'train_steps_per_second': 0.399, 'total_flos': 8946200370216960.0, 'train_loss': 2.7856999563134237, 'epoch': 4.987788331071913})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Save the model and tokenizer\ntrainer.save_model(\"./t5_chatbot_model\")\ntokenizer.save_pretrained(\"./t5_chatbot_tokenizer\")\nmodel_path = \"./t5_chatbot_model.h5\"\ntorch.save(model.state_dict(), model_path)\n\n# Save log history\nlog_history = trainer.state.log_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T06:27:20.599557Z","iopub.execute_input":"2025-04-01T06:27:20.599892Z","iopub.status.idle":"2025-04-01T06:27:24.770064Z","shell.execute_reply.started":"2025-04-01T06:27:20.599868Z","shell.execute_reply":"2025-04-01T06:27:24.768988Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Load the trained T5 model and tokenizer\nmodel_path = \"/kaggle/working/my_model/t5_chatbot_model\"\ntokenizer_path = \"/kaggle/working/t5_chatbot_tokenizer\"\n\ntokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\nmodel.eval() \n\n# Generate responses using Top-K and Top-P sampling\ndef generate_response_top_k_top_p(\n    question, model, tokenizer, max_length=64, top_k=50, top_p=0.95, temperature=1.0\n):\n    # Format the question for the model\n    formatted_question = f\"Answer the following question: {question}\"\n    \n    # Tokenize the input\n    inputs = tokenizer(\n        formatted_question,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=128,\n    )\n    \n    # Generate response with top-k and top-p sampling\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_length=max_length,\n        do_sample=True,  # Enables sampling instead of greedy/beam search\n        top_k=top_k,  # Top-K sampling\n        top_p=top_p,  # Nucleus sampling\n        temperature=temperature,  # Adjusts randomness\n        pad_token_id=tokenizer.pad_token_id,\n    )\n    \n    # Decode the generated response\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return response\n\n# Example usage\nquestion = \"What is alzheimer?\"\nresponse = generate_response_top_k_top_p(question, model, tokenizer)\nprint(\"Question:\", question)\nprint(\"Response:\", response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T06:28:12.264060Z","iopub.execute_input":"2025-04-01T06:28:12.264463Z","iopub.status.idle":"2025-04-01T06:28:15.669433Z","shell.execute_reply.started":"2025-04-01T06:28:12.264435Z","shell.execute_reply":"2025-04-01T06:28:15.668479Z"}},"outputs":[{"name":"stdout","text":"Question: What is alzheimer?\nResponse: alzheimer is a progressive cognitive impairment that occurs when your brain does not make enough decisions. it can also lead to intellectual disability and the risk of disability. some of the signs and symptoms of alzheimer include low activity and appetite. early signs and symptoms include intellectual disability, intellectual disability, and\n","output_type":"stream"}],"execution_count":31}]}